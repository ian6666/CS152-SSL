{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "path = 'Climbing-Holds-and-Volumes-14/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "test_path = \"Climbing-Holds-and-Volumes-14/train/_annotations.coco.json\"\n",
    "with open(test_path) as f:\n",
    "    file = json.loads(f.read())\n",
    "    images = file['images']\n",
    "    annotations = file['annotations']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['info', 'licenses', 'categories', 'images', 'annotations'])\n",
      "[{'id': 0, 'name': 'holds', 'supercategory': 'none'}, {'id': 1, 'name': '0', 'supercategory': 'holds'}, {'id': 2, 'name': '1', 'supercategory': 'holds'}]\n"
     ]
    }
   ],
   "source": [
    "print(file.keys())\n",
    "print(file['categories'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deleteCategoryID(annotations, category_id):\n",
    "    new_annotations = []\n",
    "    id = 0\n",
    "    for i in range(len(annotations)):\n",
    "        if int(annotations[i]['category_id']) != category_id:\n",
    "            annotations[i]['id'] = id\n",
    "            new_annotations.append(annotations[i])\n",
    "            id += 1\n",
    "          \n",
    "    return new_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotations_cleaned = deleteCategoryID(annotations, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(annotations_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file['annotations'] = annotations_cleaned\n",
    "# json_object = json.dumps(file)\n",
    "# with open(\"Climbing-Holds-and-Volumes-14/train/_annotations.coco.json\", \"w\") as outfile:\n",
    "#     outfile.write(json_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120528"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 1,\n",
       " 'image_id': 0,\n",
       " 'category_id': 1,\n",
       " 'bbox': [479, 110, 146, 110],\n",
       " 'area': 16060,\n",
       " 'segmentation': [],\n",
       " 'iscrowd': 0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 0,\n",
       " 'license': 1,\n",
       " 'file_name': '31_jpg.rf.cf19005c65737bfcea76345769f2ff5e.jpg',\n",
       " 'height': 400,\n",
       " 'width': 825,\n",
       " 'date_captured': '2023-10-13T13:22:06+00:00'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3876"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_bbox(image, bbox_coord):\n",
    "    x, y, w, h = [int(b) for b in bbox_coord]\n",
    "    return image[y:y+h, x:x+w]\n",
    "\n",
    "\n",
    "def parse_annotations(annotations):\n",
    "    img_id_to_annotations = {}\n",
    "\n",
    "    for a in annotations:\n",
    "        if a['image_id'] in img_id_to_annotations:\n",
    "            img_id_to_annotations[a['image_id']].append(a['id'])\n",
    "        else:\n",
    "            img_id_to_annotations[a['image_id']] = [a['id']]\n",
    "\n",
    "    return img_id_to_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_id_to_annotations = parse_annotations(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def getBoudingBoxForImage(imageId, img_id_to_annotations, annotations, images, path, saving_dir=None):\n",
    "    annotation_ids = img_id_to_annotations[imageId]\n",
    "    img_path =  os.path.join(path, images[imageId]['file_name'])\n",
    "    image = cv2.imread(img_path)\n",
    "\n",
    "    for a_id in annotation_ids:\n",
    "        extracted_img = extract_bbox(image, annotations[a_id]['bbox'])\n",
    "        if saving_dir:\n",
    "            try:\n",
    "                cv2.imwrite(os.path.join(saving_dir, f\"{imageId:05d}_{a_id:05d}.png\"), extracted_img)\n",
    "            except:\n",
    "                x, y, w, h = [int(b) for b in annotations[a_id]['bbox']]\n",
    "                print(annotations[a_id]['bbox'])\n",
    "                print(x, y, w, h)\n",
    "                print(image.shape)\n",
    "                print(image[y:y+h, x:x+w])\n",
    "                # print(image)\n",
    "                # print(annotations[a_id]['bbox'])\n",
    "                # print(extracted_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "getBoudingBoxForImage(0, img_id_to_annotations, annotations, images, \"Climbing-Holds-and-Volumes-14/train/\", saving_dir=\"extractedUnlabeledDataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractAllImages(interval, img_id_to_annotations, annotations, images, path = 'ClimbingHoldDetection-15/train', saving_dir=None):\n",
    "    from tqdm import tqdm\n",
    "    for i in tqdm(interval):\n",
    "        getBoudingBoxForImage(i, img_id_to_annotations, annotations, images, path, saving_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 46.95it/s]\n"
     ]
    }
   ],
   "source": [
    "# Make sure saving_dir exist\n",
    "extractAllImages(range(0,10), img_id_to_annotations, annotations, images, path = 'Climbing-Holds-and-Volumes-14/train/', saving_dir=\"extractedUnlabeledDataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# desired_class = 4\n",
    "# count = 0\n",
    "# i = 0 \n",
    "# while count < 10:\n",
    "#     if annotations[i]['category_id'] == desired_class:\n",
    "#         print(images[annotations[i]['image_id']])\n",
    "#         print(annotations[i]['id'])\n",
    "#         count += 1\n",
    "\n",
    "#     i+= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Crimp\n",
    "# 2: Jug\n",
    "# 3: Pinch\n",
    "# 4: Pocket\n",
    "# 5: Sloper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io import read_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.functional import one_hot\n",
    "import torch\n",
    "class LabeledImageDataset(Dataset):\n",
    "    def __init__(self, annotations, img_dir, num_classes, transform=None):\n",
    "        # self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.annotations = annotations\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.num_classes = num_classes\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        imageId = self.annotations[idx]['image_id']\n",
    "        img_path = os.path.join(self.img_dir, f\"{imageId:05d}_{idx:05d}.png\")\n",
    "        \n",
    "        image = read_image(img_path)\n",
    "        label = self.annotations[idx]['category_id']\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        one_hot_label = one_hot(torch.tensor(label))\n",
    "        return image, one_hot_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnlabeledImageDataset(Dataset):\n",
    "    def __init__(self, annotations, img_dir, transform=None):\n",
    "        # self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.annotations = annotations\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        imageId = self.annotations[idx]['image_id']\n",
    "        img_path = os.path.join(self.img_dir, f\"{imageId:05d}_{idx:05d}.png\")\n",
    "        \n",
    "        image = read_image(img_path)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yanhuali/opt/anaconda3/envs/jupyter/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "img_dir = \"extractedLabeledDataset\"\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64,64))\n",
    "])\n",
    "testDataset = UnlabeledImageDataset(annotations, img_dir, transform=transform)\n",
    "\n",
    "test_dataloader = DataLoader(testDataset, batch_size=2, shuffle=False)\n",
    "\n",
    "test_images = next(iter(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 64, 64])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from copy import deepcopy\n",
    "from torch.nn.functional import one_hot\n",
    "def augmentAndPredict(model, images , k, num_classes, transforms):\n",
    "    \"\"\"\n",
    "    model: model\n",
    "    images: a batch of images\n",
    "    k: k transforms\n",
    "    transforms: a list of transforms legnth k\n",
    "    \"\"\"\n",
    "    batch_size, dims, height, width = images.shape\n",
    "    all_augmented_inputs = torch.zeros([k*batch_size, dims, height, width])\n",
    "    all_preds =  torch.zeros([k*batch_size, num_classes])\n",
    "    for t in range(k):\n",
    "        transformed_images = transforms[t](images)\n",
    "        preds = model(transformed_images)\n",
    "        all_augmented_inputs[t*batch_size:(t+1)*batch_size] = transformed_images\n",
    "        all_preds[t*batch_size:(t+1)*batch_size] = preds\n",
    "    \n",
    "    return all_augmented_inputs, all_preds\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
