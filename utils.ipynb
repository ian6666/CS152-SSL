{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "path = 'ClimbingHoldDetection-15/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ClimbingHoldDetection-15/train/_annotations.coco.json') as f:\n",
    "    file = json.loads(f.read())\n",
    "    images = file['images']\n",
    "    annotations = file['annotations']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 1,\n",
       " 'image_id': 0,\n",
       " 'category_id': 5,\n",
       " 'bbox': [202, 104, 137.5, 70.5],\n",
       " 'area': 9693.75,\n",
       " 'segmentation': [],\n",
       " 'iscrowd': 0}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 0,\n",
       " 'license': 1,\n",
       " 'file_name': '20231009_133118_jpg.rf.a4403489a7a5e6fb3150293fb413dd6f.jpg',\n",
       " 'height': 640,\n",
       " 'width': 640,\n",
       " 'date_captured': '2023-11-06T01:50:53+00:00'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "382"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_bbox(image, bbox_coord):\n",
    "    x, y, w, h = [int(b) for b in bbox_coord]\n",
    "    return image[y:y+h, x:x+w]\n",
    "\n",
    "\n",
    "def parse_annotations(annotations):\n",
    "    img_id_to_annotations = {}\n",
    "\n",
    "    for a in annotations:\n",
    "        if a['image_id'] in img_id_to_annotations:\n",
    "            img_id_to_annotations[a['image_id']].append(a['id'])\n",
    "        else:\n",
    "            img_id_to_annotations[a['image_id']] = [a['id']]\n",
    "\n",
    "    return img_id_to_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_id_to_annotations = parse_annotations(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def getBoudingBoxForImage(imageId, img_id_to_annotations, annotations, images, path = 'ClimbingHoldDetection-15/train', saving_dir=None):\n",
    "    annotation_ids = img_id_to_annotations[imageId]\n",
    "    img_path =  os.path.join(path, images[imageId]['file_name'])\n",
    "\n",
    "    image = cv2.imread(img_path)\n",
    "\n",
    "\n",
    "    for a_id in annotation_ids:\n",
    "        extracted_img = extract_bbox(image, annotations[a_id]['bbox'])\n",
    "        if saving_dir:\n",
    "            cv2.imwrite(os.path.join(saving_dir, f\"{imageId:05d}_{a_id:05d}.png\"), extracted_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "getBoudingBoxForImage(0, img_id_to_annotations, annotations, images, saving_dir=\"extractedLabeledDataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractAllImages(interval, img_id_to_annotations, annotations, images, path = 'ClimbingHoldDetection-15/train', saving_dir=None):\n",
    "    from tqdm import tqdm\n",
    "    for i in tqdm(interval):\n",
    "        getBoudingBoxForImage(i, img_id_to_annotations, annotations, images, saving_dir=\"extractedLabeledDataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 109.70it/s]\n"
     ]
    }
   ],
   "source": [
    "extractAllImages(range(0,10), img_id_to_annotations, annotations, images, saving_dir=\"extractedLabeledDataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 2, 'license': 1, 'file_name': '20231009_133346_jpg.rf.a080d11ad88cfdb6f079a2c85162d447.jpg', 'height': 640, 'width': 640, 'date_captured': '2023-11-06T01:50:53+00:00'}\n",
      "66\n",
      "{'id': 2, 'license': 1, 'file_name': '20231009_133346_jpg.rf.a080d11ad88cfdb6f079a2c85162d447.jpg', 'height': 640, 'width': 640, 'date_captured': '2023-11-06T01:50:53+00:00'}\n",
      "67\n",
      "{'id': 2, 'license': 1, 'file_name': '20231009_133346_jpg.rf.a080d11ad88cfdb6f079a2c85162d447.jpg', 'height': 640, 'width': 640, 'date_captured': '2023-11-06T01:50:53+00:00'}\n",
      "75\n",
      "{'id': 5, 'license': 1, 'file_name': '20231009_133347_jpg.rf.989af4e60cffc56f5f6cc29b5a40aaff.jpg', 'height': 640, 'width': 640, 'date_captured': '2023-11-06T01:50:53+00:00'}\n",
      "144\n",
      "{'id': 5, 'license': 1, 'file_name': '20231009_133347_jpg.rf.989af4e60cffc56f5f6cc29b5a40aaff.jpg', 'height': 640, 'width': 640, 'date_captured': '2023-11-06T01:50:53+00:00'}\n",
      "146\n",
      "{'id': 5, 'license': 1, 'file_name': '20231009_133347_jpg.rf.989af4e60cffc56f5f6cc29b5a40aaff.jpg', 'height': 640, 'width': 640, 'date_captured': '2023-11-06T01:50:53+00:00'}\n",
      "151\n",
      "{'id': 7, 'license': 1, 'file_name': '20231009_133256_jpg.rf.a0c04c74fae711779ac5aa2a3f28eac7.jpg', 'height': 640, 'width': 640, 'date_captured': '2023-11-06T01:50:53+00:00'}\n",
      "215\n",
      "{'id': 10, 'license': 1, 'file_name': 'IMG-20230218-WA0025_jpg.rf.a12438a4ea4197420be681de6e5a1166.jpg', 'height': 640, 'width': 640, 'date_captured': '2023-11-06T01:50:53+00:00'}\n",
      "301\n",
      "{'id': 10, 'license': 1, 'file_name': 'IMG-20230218-WA0025_jpg.rf.a12438a4ea4197420be681de6e5a1166.jpg', 'height': 640, 'width': 640, 'date_captured': '2023-11-06T01:50:53+00:00'}\n",
      "302\n",
      "{'id': 11, 'license': 1, 'file_name': '20231009_133404-0-_jpg.rf.a17cf3d4d972156ea0761efc2480e263.jpg', 'height': 640, 'width': 640, 'date_captured': '2023-11-06T01:50:53+00:00'}\n",
      "323\n"
     ]
    }
   ],
   "source": [
    "desired_class = 4\n",
    "count = 0\n",
    "i = 0 \n",
    "while count < 10:\n",
    "    if annotations[i]['category_id'] == desired_class:\n",
    "        print(images[annotations[i]['image_id']])\n",
    "        print(annotations[i]['id'])\n",
    "        count += 1\n",
    "\n",
    "    i+= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Crimp\n",
    "# 2: Jug\n",
    "# 3: Pinch\n",
    "# 4: Pocket\n",
    "# 5: Sloper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations, img_dir, transform=None):\n",
    "        # self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.annotations = annotations\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        imageId = self.annotations[idx]['image_id']\n",
    "        img_path = os.path.join(self.img_dir, f\"{imageId:05d}_{idx:05d}.png\")\n",
    "        \n",
    "        image = read_image(img_path)\n",
    "        label = self.annotations[idx]['category_id']\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yanhuali/opt/anaconda3/envs/jupyter/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "img_dir = \"extractedLabeledDataset\"\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64,64))\n",
    "])\n",
    "trainDataset = CustomImageDataset(annotations, img_dir, transform=transform)\n",
    "\n",
    "train_dataloader = DataLoader(trainDataset, batch_size=2, shuffle=False)\n",
    "\n",
    "train_images, train_labels = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 64, 64])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 5])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 0,\n",
       "  'image_id': 0,\n",
       "  'category_id': 5,\n",
       "  'bbox': [65, 170, 129, 70.5],\n",
       "  'area': 9094.5,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0},\n",
       " {'id': 1,\n",
       "  'image_id': 0,\n",
       "  'category_id': 5,\n",
       "  'bbox': [202, 104, 137.5, 70.5],\n",
       "  'area': 9693.75,\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
